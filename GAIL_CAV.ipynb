{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/spdin/time-series-prediction-lstm-pytorch/blob/master/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FIi8t8NUTEJ"
   },
   "source": [
    "# Human driving - GAIL\n",
    "\n",
    "GAIL use specific idea like generative adversarial network (GAN).  it is a competition between the discriminator and policy.\n",
    "\n",
    "Discriminator try to distinguish the real data and fake data generated by policy. \n",
    "- Higher (D(s,a)) value: Discriminator thinks the pair is from the expert.\n",
    "- Lower (D(s,a)) value: Discriminator thinks the pair is from the policy.\n",
    "- Policy’s goal: Generate pairs that make the discriminator uncertain, ideally resulting in (D(s,a)) values that are close to those of the expert’s pairs.\n",
    " \n",
    "real data label - 1, while fake data label - 0\n",
    "\n",
    "The policy tries to fool the discriminator. it is designd to maximize E[-log(1-D(s,a))], that is maximize E[log(D(s,a))].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T17:36:55.624114Z",
     "start_time": "2021-07-17T17:36:55.042240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yannis\\OneDrive - Georgia Institute of Technology\\Desktop\\New folder\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Utils.Environment_LC import ENVIRONMENT\n",
    "from Utils.PPO import PPO\n",
    "from Utils.GAIL import DISCRIMINATOR_FUNCTION   # NEW compare to PPO code\n",
    "\n",
    "Path = os.getcwd()\n",
    "print(Path)\n",
    "PATH = \"Trained_model/GAIL_0.pth\"\n",
    "\n",
    "#CUDA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    expert_traj = np.load(\"Expert_trajectory/expert_traj.npy\", allow_pickle=True)\n",
    "except:\n",
    "    print(\"Train, generate and save expert trajectories\")\n",
    "    assert False\n",
    "\n",
    "try:\n",
    "    testing_traj = np.load(\"Expert_trajectory/testing_traj.npy\", allow_pickle=True)\n",
    "except:\n",
    "    print(\"Train, generate and save expert trajectories\")\n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_para = 'normal'\n",
    "B_para = '2000'\n",
    "Env = ENVIRONMENT(\n",
    "    para_B                          = B_para,                               \n",
    "    para_A                          = A_para, \n",
    "    noise                           = False)\n",
    "Model_B = Env.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T17:37:00.215453Z",
     "start_time": "2021-07-17T17:36:59.044591Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "actor_acc.0.weight \t torch.Size([64, 11])\n",
      "actor_acc.0.bias \t torch.Size([64])\n",
      "actor_acc.2.weight \t torch.Size([64, 64])\n",
      "actor_acc.2.bias \t torch.Size([64])\n",
      "actor_acc.4.weight \t torch.Size([1, 64])\n",
      "actor_acc.4.bias \t torch.Size([1])\n",
      "actor_yaw.0.weight \t torch.Size([64, 11])\n",
      "actor_yaw.0.bias \t torch.Size([64])\n",
      "actor_yaw.2.weight \t torch.Size([64, 64])\n",
      "actor_yaw.2.bias \t torch.Size([64])\n",
      "actor_yaw.4.weight \t torch.Size([1, 64])\n",
      "actor_yaw.4.bias \t torch.Size([1])\n",
      "critic.0.weight \t torch.Size([64, 11])\n",
      "critic.0.bias \t torch.Size([64])\n",
      "critic.2.weight \t torch.Size([64, 64])\n",
      "critic.2.bias \t torch.Size([64])\n",
      "critic.4.weight \t torch.Size([1, 64])\n",
      "critic.4.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "K_epochs = 16               # update policy for K epochs\n",
    "eps_clip = 0.25             # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.001            # learning rate for actor network\n",
    "lr_critic = 0.001           # learning rate for critic network\n",
    "\n",
    "has_continuous_action_space = True\n",
    "\n",
    "update_episode = 2#8\n",
    "\n",
    "total_episodes   = 400\n",
    "save_model_freq = 1\n",
    "\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 2 \n",
    "action_bound = 2\n",
    "\n",
    "ppo_agent  = PPO(state_dim, action_dim, action_bound, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.4)\n",
    "\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in ppo_agent.policy.state_dict():\n",
    "    print(param_tensor, \"\\t\", ppo_agent.policy.state_dict()[param_tensor].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW compare to PPO code\n",
    "# Discriminator\n",
    "D_epochs    = 16     # update discriminator for D epochs\n",
    "\n",
    "lr_gail = 0.0001\n",
    "\n",
    "expert_sample_size = expert_traj.shape[0] #len(expert_traj) \n",
    "\n",
    "Discriminator = DISCRIMINATOR_FUNCTION(state_dim, action_dim, lr_gail, D_epochs, expert_traj, expert_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m Env\u001b[38;5;241m.\u001b[39mrun(action) \u001b[38;5;66;03m# run human behavior\u001b[39;00m\n\u001b[0;32m     70\u001b[0m state_next, _ \u001b[38;5;241m=\u001b[39m Env\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m---> 72\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mDiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m Dat_tmp \u001b[38;5;241m=\u001b[39m Env\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(Dat_tmp[t,\u001b[38;5;241m25\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mlane_wid) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m LC_end_pos \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(Dat_tmp[t,\u001b[38;5;241m26\u001b[39m]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m LC_end_yaw:\n",
      "File \u001b[1;32mc:\\Users\\Yannis\\OneDrive - Georgia Institute of Technology\\Desktop\\New folder\\Utils\\GAIL.py:38\u001b[0m, in \u001b[0;36mDISCRIMINATOR_FUNCTION.reward\u001b[1;34m(self, state, action, epsilon)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-40\u001b[39m):\n\u001b[1;32m---> 38\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39mexpand_dims(action, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;66;03m# epsilon avoids log(0)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LC_end_pos = 0.5      # position deviation   0.5\n",
    "LC_end_yaw = 0.005      # speed     0.02\n",
    "PARAS = ['aggressive', 'normal', 'cautious']\n",
    "\n",
    "path_idx    = 0\n",
    "std = 0.7\n",
    "Time_len = 500\n",
    "lane_wid = 3.75               \n",
    "veh_len  = 5.0\n",
    "v_0      = 30   \n",
    "\n",
    "#for episode in tqdm(range(total_episodes)):\n",
    "for episode in range(total_episodes):\n",
    "    print(episode)\n",
    "\n",
    "    # Initialize the data\n",
    "    states      = []\n",
    "    actions     = []\n",
    "\n",
    "    for A_para in PARAS:  # interact with different driver types iteratively, aggressive, normal, and cautious\n",
    "        B_para = A_para\n",
    "\n",
    "        # Environment\n",
    "        Env.AB_update(A_para,B_para)\n",
    "        Env.reset()    \n",
    "        LC_start = False   \n",
    "        LC_starttime = 0\n",
    "        LC_endtime   = 0\n",
    "        LC_mid       = 0\n",
    "\n",
    "        for t in range(1, Time_len):  \n",
    "            s_t, env_t = Env.observe()       #Observation        \n",
    "            if t != env_t + 1:\n",
    "                print('warning: time inconsistency!')\n",
    "\n",
    "            Dat = Env.read()\n",
    "\n",
    "            #Lane change indication\n",
    "            if Dat[t-1,24]!=0 and LC_start == False and LC_starttime == 0:                 # if LC is true at the end of last time step\n",
    "                LC_start = True  \n",
    "                LC_starttime = t\n",
    "            # finish lane change - stop in the center of the target lane\n",
    "            elif abs(Dat[t-1,25] - 0.5*lane_wid) <= LC_end_pos and abs(Dat[t-1,26]) <= LC_end_yaw and LC_start == True and LC_endtime == 0:       \n",
    "                LC_start = False\n",
    "                LC_endtime   = t\n",
    "              \n",
    "            # B cross the line    \n",
    "            if Dat[t-1,25]<=lane_wid and LC_mid==0:\n",
    "                LC_mid = t\n",
    "\n",
    "            # Low-level task: action              \n",
    "            if LC_start == False:\n",
    "                # longitudinal\n",
    "                if Dat[t-1,25] > lane_wid:     # B in lane 2, B follow F   \n",
    "                    act_0 = Model_B(Dat[t-1,9] - Dat[t-1,12] - veh_len, Dat[t-1,13], v_0, (Dat[t-1,10] - Dat[t-1,13], Dat[t-1,11] - Dat[t-1,14]))\n",
    "                elif Dat[t-1,25] <= lane_wid:  # B cross the line, B follow E\n",
    "                    act_0 = Model_B(Dat[t-1,0] - Dat[t-1,12] - veh_len, Dat[t-1,13], v_0, (Dat[t-1,1] - Dat[t-1,13], Dat[t-1,2] - Dat[t-1,14]))\n",
    "                \n",
    "                \n",
    "                # lateral\n",
    "                act_1 = 0                   # yaw rate\n",
    "                action_B = [act_0, act_1*10]\n",
    "\n",
    "                Env.run(action_B)\n",
    "            else:\n",
    "                state, _ = Env.observe()\n",
    "\n",
    "                action = ppo_agent.select_action(state)\n",
    "                Env.run(action) # run human behavior\n",
    "                state_next, _ = Env.observe()\n",
    "\n",
    "                reward = Discriminator.reward(state, action)\n",
    "\n",
    "                Dat_tmp = Env.read()\n",
    "                if abs(Dat_tmp[t,25] - 0.5*lane_wid) <= LC_end_pos and abs(Dat_tmp[t,26]) <= LC_end_yaw:\n",
    "                    done = True\n",
    "                else:\n",
    "                    done = False\n",
    "                                    \n",
    "                ppo_agent.buffer.rewards.append(reward)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "            \n",
    "    ############# Policy updates ##########\n",
    "    ppo_agent.update()\n",
    "\n",
    "    # Discriminator\n",
    "    states    = torch.FloatTensor(np.array(states)).squeeze().to(device)\n",
    "    actions   = torch.FloatTensor(np.array(actions)).to(device)\n",
    "    Discriminator.update(ppo_agent, states, actions)\n",
    "\n",
    "    ############# Record episode results ##########\n",
    "    # write conditions you want to save the trained model during the training\n",
    "    # Code\n",
    "    \n",
    "    if episode % save_model_freq  == 0:\n",
    "        path_idx   += 1\n",
    "        max_score = -np.inf\n",
    "        PATH       = \"Trained_model/GAIL_\"+str(path_idx)+\".pth\"\n",
    "        ppo_agent.save(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Time Series Prediction with LSTM Using PyTorch",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169.488px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "512.4px",
    "left": "643.8px",
    "right": "20px",
    "top": "26px",
    "width": "682px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
