{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/spdin/time-series-prediction-lstm-pytorch/blob/master/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FIi8t8NUTEJ"
   },
   "source": [
    "# Human driving - [GAIL](https://arxiv.org/abs/1606.03476)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T17:36:55.624114Z",
     "start_time": "2021-07-17T17:36:55.042240Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Utils.Environment_LC import ENVIRONMENT\n",
    "from Utils.PPO import PPO\n",
    "from Utils.GAIL import DISCRIMINATOR_FUNCTION   # NEW compare to PPO code\n",
    "\n",
    "Path = os.getcwd()\n",
    "print(Path)\n",
    "PATH = \"Trained_model/GAIL_0.pth\"\n",
    "\n",
    "#CUDA\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW compare to PPO code\n",
    "# Training\n",
    "try:\n",
    "    expert_traj = np.load(\"Expert_trajectory/expert_traj.npy\", allow_pickle=True)\n",
    "except:\n",
    "    print(\"Train, generate and save expert trajectories\")\n",
    "    assert False\n",
    "\n",
    "# Testing\n",
    "try:\n",
    "    testing_traj = np.load(\"Expert_trajectory/testing_traj.npy\", allow_pickle=True)\n",
    "except:\n",
    "    print(\"Train, generate and save expert trajectories\")\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIDM_do = True\n",
    "A_para = 'normal'\n",
    "B_para = '2000'\n",
    "Env = ENVIRONMENT(\n",
    "    para_B                          = B_para,                               \n",
    "    para_A                          = A_para, \n",
    "    noise                           = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T17:37:00.215453Z",
     "start_time": "2021-07-17T17:36:59.044591Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "actor_acc.0.weight \t torch.Size([64, 11])\n",
      "actor_acc.0.bias \t torch.Size([64])\n",
      "actor_acc.2.weight \t torch.Size([64, 64])\n",
      "actor_acc.2.bias \t torch.Size([64])\n",
      "actor_acc.4.weight \t torch.Size([1, 64])\n",
      "actor_acc.4.bias \t torch.Size([1])\n",
      "actor_yaw.0.weight \t torch.Size([64, 11])\n",
      "actor_yaw.0.bias \t torch.Size([64])\n",
      "actor_yaw.2.weight \t torch.Size([64, 64])\n",
      "actor_yaw.2.bias \t torch.Size([64])\n",
      "actor_yaw.4.weight \t torch.Size([1, 64])\n",
      "actor_yaw.4.bias \t torch.Size([1])\n",
      "critic.0.weight \t torch.Size([64, 11])\n",
      "critic.0.bias \t torch.Size([64])\n",
      "critic.2.weight \t torch.Size([64, 64])\n",
      "critic.2.bias \t torch.Size([64])\n",
      "critic.4.weight \t torch.Size([1, 64])\n",
      "critic.4.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "K_epochs = 16              # update policy for K epochs\n",
    "eps_clip = 0.15             # clip parameter for PPO\n",
    "gamma = 0.95                # discount factor\n",
    "\n",
    "lr_actor = 0.001#0.001            # learning rate for actor network\n",
    "lr_critic = 0.001#0.001           # learning rate for critic network\n",
    "\n",
    "has_continuous_action_space = True\n",
    "\n",
    "update_episode = 2\n",
    "\n",
    "total_episodes   =  1200\n",
    "# action_std_decay_freq = 10\n",
    "# action_std_decay_rate = 0.01\n",
    "# min_action_std = 0.01  \n",
    "save_model_freq = 1\n",
    "\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 2 \n",
    "action_bound = 2\n",
    "\n",
    "ppo_agent  = PPO(state_dim, action_dim, action_bound, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.4)\n",
    "\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in ppo_agent.policy.state_dict():\n",
    "    print(param_tensor, \"\\t\", ppo_agent.policy.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW compare to PPO code\n",
    "# Discriminator\n",
    "D_epochs    = 8     # update discriminator for D epochs\n",
    "\n",
    "lr_gail = 0.0005\n",
    "\n",
    "expert_sample_size = expert_traj.shape[0] #len(expert_traj) \n",
    "\n",
    "Discriminator = DISCRIMINATOR_FUNCTION(state_dim, action_dim, lr_gail, D_epochs, expert_traj, expert_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LC_end_pos = 0.5      # lateral position deviation\n",
    "LC_end_yaw = 0.005    # yaw angle deviation \n",
    "PARAS = ['aggressive', 'normal', 'cautious']\n",
    "\n",
    "path_idx    = 0\n",
    "Time_len = 500\n",
    "lane_wid = 3.75               \n",
    "veh_len  = 5.0\n",
    "v_0      = 30   \n",
    "\n",
    "#for episode in tqdm(range(total_episodes)):\n",
    "for episode in range(total_episodes):\n",
    "    \n",
    "    # Collect PPO outputs\n",
    "    states      = [] # NEW compare to PPO code\n",
    "    actions     = [] # NEW compare to PPO code\n",
    "    \n",
    "    for iter in range(3):\n",
    "        A_para = PARAS[iter]   # interact with different driver types iteratively, aggressive, normal, and cautious\n",
    "    \n",
    "        # Environment    \n",
    "        Env.reset()   \n",
    "        LC_start = False   \n",
    "        LC_starttime = 0\n",
    "        LC_endtime   = 0\n",
    "        LC_mid       = 0\n",
    "\n",
    "        for t in range(1, Time_len):  \n",
    "            s_t, env_t = Env.observe()       #Observation        \n",
    "            if t != env_t + 1:               # check time consistency between Env and simulation codee\n",
    "                print('warning: time inconsistency!')\n",
    "\n",
    "            Dat = Env.read()                 # Read ground-truth information\n",
    "\n",
    "            #Lane change indication\n",
    "            if Dat[t-1,24]!=0 and LC_start == False and LC_starttime == 0:                 # if LC is true at the end of last time step\n",
    "                LC_start = True  \n",
    "                LC_starttime = t\n",
    "            # finish lane change - stop in the center of the target lane\n",
    "            elif abs(Dat[t-1,25] - 0.5*lane_wid) <= LC_end_pos and abs(Dat[t-1,26]) <= LC_end_yaw and LC_start == True and LC_endtime == 0:       \n",
    "                LC_start = False\n",
    "                LC_endtime   = t\n",
    "            # out of boundary\n",
    "            elif (Dat[t-1,25] <= - lane_wid or Dat[t-1,25] > 2.0 * lane_wid) and LC_start == True and LC_endtime == 0:       \n",
    "                LC_start = False\n",
    "                LC_endtime   = t\n",
    "            \n",
    "            # B cross the line    \n",
    "            if Dat[t-1,25]<=lane_wid and LC_mid==0:\n",
    "                LC_mid = t         # record the time cross lane-marking\n",
    "\n",
    "            # Low-level task: action              \n",
    "            if LC_start == False:\n",
    "                # longitudinal\n",
    "                if Dat[t-1,25] > lane_wid:    # B in lane 2, B follwo F\n",
    "                    act_0 = Env.IDM_B(Dat[t-1,13], Dat[t-1,13] - Dat[t-1,10], Dat[t-1,9] - Dat[t-1,12] - veh_len) #IDM\n",
    "                elif Dat[t-1,25] <= lane_wid:   # B cross the line, B follow E\n",
    "                    act_0 = Env.IDM_B(Dat[t-1,13], Dat[t-1,13] - Dat[t-1,1], Dat[t-1,0] - Dat[t-1,12] - veh_len) #IDM\n",
    "                \n",
    "                # lateral\n",
    "                act_1 = 0                   # yaw rate\n",
    "                \n",
    "                action = [act_0, act_1]\n",
    "                Env.run(action)\n",
    "            \n",
    "            else:\n",
    "                state, _ = Env.observe()\n",
    "\n",
    "                action = ppo_agent.select_action(state)\n",
    "                Env.run(action) # run human behavior\n",
    "                state_next, _ = Env.observe()\n",
    "\n",
    "                reward = Discriminator.reward(state, action)    # reward from the discriminator    # NEW compare to PPO code        \n",
    "                \n",
    "                if t == Time_len - 1:\n",
    "                    done = True\n",
    "                else:\n",
    "                    done = False\n",
    "                    \n",
    "                ppo_agent.buffer.rewards.append(reward)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "\n",
    "                # Collect PPO outputs\n",
    "                states.append(state) # NEW compare to PPO code\n",
    "                actions.append(action) # NEW compare to PPO code\n",
    "\n",
    "        \n",
    "    # Policy\n",
    "    if episode % update_episode  == 0:\n",
    "        # PPO\n",
    "        ppo_agent.update()\n",
    "\n",
    "        # Discriminator\n",
    "        states    = torch.FloatTensor(np.array(states)).squeeze().to(device) # NEW compare to PPO code\n",
    "        actions   = torch.FloatTensor(np.array(actions)).to(device) # NEW compare to PPO code\n",
    "        Discriminator.update(ppo_agent, states, actions) # NEW compare to PPO code\n",
    "\n",
    "    ############# Record episode results ##########\n",
    "    # write conditions you want to save the trained model during the training\n",
    "    # Code\n",
    "    \n",
    "    if episode % save_model_freq  == 0:\n",
    "        path_idx   += 1\n",
    "        max_score = -np.inf\n",
    "        PATH       = \"Trained_model/GAIL_\"+str(path_idx)+\".pth\"\n",
    "        ppo_agent.save(PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "testing_trajectory = testing_traj[0]\n",
    "for i in range(1,len(testing_traj)):\n",
    "    testing_trajectory = np.concatenate((testing_trajectory, testing_traj[i]), axis=0)\n",
    "\n",
    "testing_trajectory = testing_trajectory.reshape(-1,state_dim+action_dim+state_dim)\n",
    "testing_trajectory = torch.FloatTensor(testing_trajectory).to(device)\n",
    "testing_states, testing_actions, testing_states_next = torch.split(testing_trajectory, (state_dim,action_dim,state_dim), dim=1)\n",
    "\n",
    "expert_trajectory = expert_traj[0]\n",
    "for i in range(1,len(expert_traj)):\n",
    "    expert_trajectory = np.concatenate((expert_trajectory, expert_traj[i]), axis=0)\n",
    "\n",
    "expert_trajectory = expert_trajectory.reshape(-1,state_dim+action_dim+state_dim)\n",
    "expert_trajectory = torch.FloatTensor(expert_trajectory).to(device)\n",
    "expert_states, expert_actions, expert_states_next = torch.split(expert_trajectory, (state_dim,action_dim,state_dim), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472 0.05902663990855217 Trained_model/GAIL_473.pth\n"
     ]
    }
   ],
   "source": [
    "MSE_test = []\n",
    "idx = []\n",
    "for item in range(0,total_episodes):\n",
    "    PATH  = \"Trained_model/GAIL_\"+str(item)+\".pth\"\n",
    "    \n",
    "    if os.path.exists(PATH) == False:\n",
    "        continue\n",
    "\n",
    "    ppo_agent.load(PATH) \n",
    "    ppo_agent.set_action_std(0.00000000001) \n",
    "    ppo_agent.policy_old.eval()\n",
    "    #print('PATH:', PATH)\n",
    "\n",
    "    actions,_,_ = ppo_agent.policy_old.act(testing_states)\n",
    "    \n",
    "    mse = loss(actions, testing_actions).item()\n",
    "    MSE_test.append(mse)\n",
    "    idx.append(item)\n",
    "\n",
    "print(np.argmin(MSE_test), MSE_test[np.argmin(MSE_test)], \"Trained_model/GAIL_\"+str(idx[np.argmin(MSE_test)])+\".pth\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_plot(actions, testing_actions):\n",
    "    plt.figure(dpi=60, figsize=[36, 4]) \n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(actions[:,0],c='b', linewidth=1, label = 'AIRL')\n",
    "    plt.plot(testing_actions[:,0],c='r', linewidth=1, label = 'Human')\n",
    "    plt.gca().set_title('Acceleration')\n",
    "    plt.xlabel('Time (0.1s)')\n",
    "    plt.ylabel('Acc (m/s^2)')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(actions[:,1],c='b', linewidth=1, label = 'AIRL')\n",
    "    plt.plot(testing_actions[:,1],c='r', linewidth=1, label = 'Human')\n",
    "    # plt.axhline(y=-0.2, c='m', linestyle='--', linewidth=1, label = 'lb')\n",
    "    # plt.axhline(y=0.2, c='m', linestyle='--', linewidth=1, label ='ub')\n",
    "    plt.gca().set_title('Yaw rate')\n",
    "    plt.xlabel('Time (0.1s)')\n",
    "    plt.ylabel('Yaw rate (rad/s)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m MSE_PATH \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained_model/GAIL_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[43midx\u001b[49m[np\u001b[38;5;241m.\u001b[39margmin(MSE_test)])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrained_model/GAIL_1000.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m ppo_agent\u001b[38;5;241m.\u001b[39mload(PATH) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    }
   ],
   "source": [
    "MSE_PATH = []\n",
    "PATH = \"Trained_model/GAIL_\"+str(idx[np.argmin(MSE_test)])+\".pth\"\n",
    "PATH = 'Trained_model/GAIL_1000.pth'\n",
    "ppo_agent.load(PATH) \n",
    "ppo_agent.set_action_std(0.00000000001) \n",
    "ppo_agent.policy_old.eval()\n",
    "\n",
    "for i in range(0,len(testing_traj)):\n",
    "    testing_states, testing_actions, _ = torch.split(torch.FloatTensor(testing_traj[i]).to(device), (state_dim,action_dim,state_dim), dim=1)\n",
    "\n",
    "    actions,_,_ = ppo_agent.policy_old.act(testing_states)\n",
    "    actions = actions\n",
    "    mse = loss(actions, testing_actions).item()\n",
    "    MSE_PATH.append(mse)\n",
    "    \n",
    "    # temp1 = loss(actions[:,0], testing_actions[:,0]).item()\n",
    "    # temp2 = loss(actions[:,1], testing_actions[:,1]).item()\n",
    "    # print(\" ACC      Loss: %1.6f\" % (temp1))\n",
    "    # print(\" Yaw rate Loss: %1.6f\" % (temp2))\n",
    "\n",
    "    print(mse)\n",
    "    Test_plot(actions.cpu(), testing_actions.cpu())\n",
    "sum(MSE_PATH)/len(MSE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Time Series Prediction with LSTM Using PyTorch",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "cav_gail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169.488px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "512.4px",
    "left": "643.8px",
    "right": "20px",
    "top": "26px",
    "width": "682px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
